#train models on datasets which are generated by generate_my_dataset.py
import os
current_path = os.getcwd()
import os.path as osp
import sys 
import torch
import torch.nn.functional as F
import argparse
import tqdm
from torch_geometric.loader import DataLoader
from datetime import datetime, time, date
from GNNs.model_selector import model_selector, new_model_selector
from Datasets.dataset_loader import dataset_selector, new_dataset_selector
from datetime import datetime
from sklearn.model_selection import train_test_split
from utils.split_dataset import split_dataset
from utils.train_test import train_graph, test_graph
from utils.process_config import gen_model_config
import yaml
#from ExplanationEvaluation.datasets.train_test_split import train_test_spliter
#from utils.syn_datasets_from_numpy import SynDatasetsFromNumpy
#from utils.datasets_from_dig import SynDatasetsFromDIG
#device = torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')


def parse_args():
    parser = argparse.ArgumentParser(description="Train GNN Model")
    parser.add_argument('--type', type=str, default='GCN',
                        help='The type of the model, choose from GCN, MLP, GINE, GCN3PoolNorm, GCN3PoolFun')
    parser.add_argument('--layers', type=int, default=3,
                        help='The number of layers in the model')
    parser.add_argument('--hidden_size', type=int, default=20,
                        help='The number of hidden units in the model')
    parser.add_argument('--dataset', type=str, default='ba2',
                        help='Dataset to use.')
    parser.add_argument('--cuda', type=int, default=0,
                        help='GPU device.')
    parser.add_argument('--save_path', type=str, default=osp.join(osp.dirname(__file__), 'SavedModels'),)
    parser.add_argument('--epoch', type=int, default=1000, 
                        help='Number of epoch.')
    parser.add_argument('--lr', type=float, default= 1e-3,
                        help='Learning rate.')
    parser.add_argument('--wd', type=float, default=0,
                        help = 'Weight decay for optimizer')
    parser.add_argument('--batch_size', type=int, default=256,
                        help='Batch size.')
    parser.add_argument('--seed', type=int, default=8,
                        help='Random seed')
    parser.add_argument('--criterion', type=str, default='acc',
                        help='Criterion to use, now can use acc, prec, loss, recall, f1.')
    parser.add_argument('--norm', type=str, default= 'LayerNorm',
                        help='Normalization method to use, now can use LayerNorm, InstanceNorm, BatchNorm is not recommended, use None to use the default config.')
    parser.add_argument('--pool', type=str, default= None,
                        help='Pooling method to use, now can use global_max_pool, global_mean_pool, global_add_pool, use None to use the default config.')
    parser.add_argument('--resample', type=float, default= None,
                        help='Resample the negative samples to balance the dataset, only used for ac or fc dataset')
    parser.add_argument('--stop_threshold', type=float, default=1.0,
                        help='The threshold to stop the training')
    #parser.add_argument('--hidden_size', type=int, default=10,
                        #help='the hidden channels in convolutional layers')
    #parser.add_argument('--l2_lambda', type=float, default=0.01,
                        #help='l2 regularization weight')#won't be used because we can use weight_decay to do the same thing
    return parser.parse_args()

def main():
    # train graph classification models by loss
    current_time = datetime.now()
    time = current_time.strftime("%m%d%H%M%S")
    args = parse_args()
    device = torch.device(f"cuda:{args.cuda}" if torch.cuda.is_available() else 'cpu')
    print(f'the device is {device}')
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed(args.seed ** 2)
    #dataset = dataset_selector(args.dataset, args.folder)
    config = yaml.safe_load(open('./Configs/TrainGNN/config.yaml', 'r'))
    config['Pool'] = args.pool if args.pool is not None else config['Pool']
    config['Norm'] = args.norm if args.norm is not None else config['Norm']
    dataset = new_dataset_selector(args.dataset, resample = args.resample)
    model = new_model_selector(args.dataset, layers = args.layers, hidden_size = args.hidden_size, types = args.type, config = config)
    model = model.to(device)
    optimizer=torch.optim.Adam(model.parameters(),lr=args.lr,weight_decay=args.wd)
    criterion=torch.nn.CrossEntropyLoss(reduction='mean')
    print(f'the start time is {time}')
    gen_model_config(args.layers, args.type, args.hidden_size, f'{args.dataset}_{time}', args.norm)
    #shuffled_set = dataset.shuffle()
    #split the dataset into train and test, maybe use a function to do this

    train_index, test_index = split_dataset(len(dataset), ratio = 0.8)
    train_set = torch.utils.data.Subset(dataset, train_index)
    test_set = torch.utils.data.Subset(dataset, test_index)
    print(f'the length of train_set is {len(train_set)}, the length of test_set is {len(test_set)}')

    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True)
    test_loader = DataLoader(test_set, batch_size= len(test_set), shuffle=False, num_workers=4, pin_memory=True)



    best_pfm = [0.0, 0.0, -1000.0, 0.0, 0.0] #accuracy, precision, negative loss, recall, f1
    for i in range(args.epoch):
        train_graph(model, train_loader, optimizer, criterion, device=device)
        test_pfm = test_graph(model, test_loader, criterion, device)

        print(f'Epoch {i}, Test Accuracy: {test_pfm[0]}, Precision: {test_pfm[1]}, Loss: {-test_pfm[2]}, Recall: {test_pfm[3]}, F1: {test_pfm[4]}')
        best_pfm, stop_training = CompareAndSave(test_pfm, best_pfm, args.criterion, model, osp.join(args.save_path, f'{args.dataset}_{time}.pth'), args.stop_threshold)
        if stop_training:
            print(f'stop training at epoch {i}')
            break
            
    print(f' the final accuracy is {best_pfm[0]}, the final precision is {best_pfm[1]}, the final loss is {-best_pfm[2]}')
    print(f' the final recall is {best_pfm[3]}, the final f1 is {best_pfm[4]}')
    print(f'start time is {time}')

def CompareAndSave(test_pfm, best_pfm, criterion, model, path, stop_threshold):
    StrToNum = {'acc':0, 'prec':1, 'loss':2, 'recall':3, 'f1':4}#neg_loss is negative loss, since loss is the smaller the better, while acc and precision are the larger the better
    if test_pfm[StrToNum[criterion]] > best_pfm[StrToNum[criterion]]:
        best_pfm = test_pfm
        torch.save(model.state_dict(), path)
        print(f'saved the model !')
    stop_training = True if test_pfm[StrToNum[criterion]] > stop_threshold else False

    return best_pfm, stop_training


if __name__ == '__main__':
    main()

'''
command line example:
CUDA_VISIBLE_DEVICES=0 python TrainGraph.py --dataset=mutagenicity --lr=1e-3 --epoch=1000 --seed=8 --type=GCN --layers=3 --hidden_size=20
CUDA_VISIBLE_DEVICES=0 python TrainGraph.py --dataset=bam --lr=1e-3 --epoch=5000 --seed=88 --type=GCN --layers=3 --hidden_size=20
CUDA_VISIBLE_DEVICES=0 python TrainGraph.py --dataset=mnist --lr=1e-3 --epoch=1000 --seed=8 --layers=3 --hidden_size=20 --batch_size=1024 --criterion=acc --type=GCN3PoolNorm
CUDA_VISIBLE_DEVICES=2 python TrainGraph.py --dataset=ac --lr=1e-3 --epoch=1000 --seed=8 --layers=3 --hidden_size=20 --batch_size=128 --criterion=f1 --type=GCN3PoolNorm --norm=LayerNorm
for neoba2 dataset, need to ignore the warnings
CUDA_VISIBLE_DEVICES=2 python -W ignore TrainGraph.py --dataset=neoba2 --lr=1e-3 --epoch=2000 --seed=8 --layers=3 --hidden_size=20 --batch_size=256 --criterion=acc --type=GCN3PoolNorm --norm=LayerNorm
'''